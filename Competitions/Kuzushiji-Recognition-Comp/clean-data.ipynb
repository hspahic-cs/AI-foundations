{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/competitions/kuzushiji-recognition/data\n",
    "import numpy as np\n",
    "import os \n",
    "import cv2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining CONST\n",
    "DIR = \"/home/harris/Projects/ML/Datasets/Kuzushiji-Recognition/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempt 1\n",
    "def segment_image_using_contours(path, file):\n",
    "    # Read the image\n",
    "    img = cv2.imread(DIR + path + file, cv2.IMREAD_GRAYSCALE)\n",
    "    \n",
    "    # Adaptive thresholding\n",
    "    binary_img = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY_INV)\n",
    "    \n",
    "    # Morphological Operations\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3,3))\n",
    "    dilated_img = cv2.dilate(binary_img, kernel, iterations=1)\n",
    "    processed_img = cv2.erode(dilated_img, kernel, iterations=1)\n",
    "    \n",
    "    # Find contours\n",
    "    contours, _ = cv2.findContours(processed_img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if not os.path.exists(DIR + \"\\cleaned_train_images\"):\n",
    "       os.makedirs(DIR + \"\\cleaned_train_images\")\n",
    "    reconstructed_img = 255 * np.ones_like(img)\n",
    "    \n",
    "    # Loop through the contours and extract individual kanji characters\\n,\n",
    "    for i, contour in enumerate(contours):\n",
    "        print(contour.shape)\n",
    "        if 200 <= cv2.contourArea(contour) <= 4000:\n",
    "            x, y, w, h = cv2.boundingRect(contour)\n",
    "            kanji = img[y:y+h, x:x+w]\n",
    "            reconstructed_img[y:y+h, x:x+w] = kanji\n",
    "    cv2.imwrite(os.path.join(DIR + \"\\cleaned_train_images\" + \"\\reconstructed_image.png\"), reconstructed_img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/harris/Projects/ML/AI-foundations/Competitions/Kuzushiji-Recognition-Comp\n"
     ]
    }
   ],
   "source": [
    "# Attempt 2 at segmenting images\n",
    "    \n",
    "def segment_image_using_edges(path, file):\n",
    "    # Read the image\n",
    "    img = cv2.imread(DIR + path + file, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    # Adaptive thresholding\n",
    "    edges = cv2.Canny(img, 50, 150)\n",
    "\n",
    "    # Morphological Operations\n",
    "    # kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3,3))\n",
    "    # dilated_img = cv2.dilate(binary_img, kernel, iterations=1)\n",
    "    # processed_img = cv2.erode(dilated_img, kernel, iterations=1)\n",
    "\n",
    "    # Find contours\n",
    "    contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    if not os.path.exists(DIR + \"/cleaned_train_images\"):\n",
    "        os.makedirs(DIR + \"/cleaned_train_images\")\n",
    "    \n",
    "    reconstructed_img = 255 * np.ones_like(img)\n",
    "    \n",
    "    # Loop through the contours and extract individual kanji characters\n",
    "    for i, contour in enumerate(contours):\n",
    "        print(contour.shape)\n",
    "        if 200 <= cv2.contourArea(contour) <= 4000:\n",
    "            x, y, w, h = cv2.boundingRect(contour)\n",
    "            kanji = img[y:y+h, x:x+w]\n",
    "            reconstructed_img[y:y+h, x:x+w] = kanji\n",
    "\n",
    "    cv2.imwrite(os.path.join(DIR + \"/cleaned_train_images\" + \"/reconstructed_image_edges.png\"), reconstructed_img)\n",
    "\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Attempt 3\n",
    "\n",
    "def VisualizeKuzushiji(imagePath):\n",
    "    # Reading image & preparing to draw,\n",
    "    img = cv2.imread(imagePath)\n",
    "    imsource = Image.open(imagePath)\n",
    "    char_draw = ImageDraw.Draw(imsource)\n",
    "\n",
    "    # Preprocessing,\n",
    "    im_grey = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    ret, im_th = cv2.threshold(im_grey, 130, 255, cv2.THRESH_BINARY_INV)\n",
    "    ctrs, _ = cv2.findContours(im_th.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    ''',\n",
    "    RETR_EXTERNAL b/c we only want the contours of the parent characters to make a bounding,\n",
    "    box, we don't need a sub contour for each stroke inside of a character,\n",
    "    CHAIN_APPROX_SIMPLE b/c we only need the bounding box,\n",
    "    '''\n",
    "\n",
    "    rects = [cv2.boundingRect(ctr) for ctr in ctrs]\n",
    "    display_img = im_th\n",
    "    max_width = []\n",
    "\n",
    "    for rect in rects:\n",
    "        x, y, w, h = rect\n",
    "        # Characters for the most part should only take 1/6th of image,\n",
    "        if h * w > 6000 and w < 500:\n",
    "            display_img = cv2.rectangle(display_img, (x, y), (x+w, y+h), (255, 255, 255), 2)\n",
    "            \n",
    "\n",
    "   \n",
    "    plt.figure(figsize=(30,30))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.title(\"Detection of Kuzushiji\",fontsize=20)\n",
    "    plt.imshow(display_img)\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.title(\"Training data\",fontsize=20)\n",
    "    plt.imshow(VisualizeTraining(imagePath))\n",
    "\n",
    "\n",
    "    return char_draw\n",
    "\n",
    "def VisualizeTraining(imagePath):\n",
    "    '''\n",
    "    Display the training image with the bounding box of the characters\n",
    "    '''\n",
    "\n",
    "    img = cv2.imread(imagePath)\n",
    "\n",
    "    def chunks(lst, n):\n",
    "        for i in range(0, len(lst), n):\n",
    "            yield lst[i:i + n]\n",
    "\n",
    "    data = \"U+306F 1231 3465 133 53 U+304C 275 1652 84 69 U+3044 1495 1218 143 69 U+3051 220 3331 53 91 U+306B 911 1452 61 92 U+306B 927 3445 71 92 U+306E 904 2879 95 92 U+5DE5 1168 1396 187 95 U+3053 289 3166 69 97 U+4E09 897 3034 121 107 U+306E 547 1912 141 108 U+3084 1489 2675 151 109 U+3068 1561 2979 55 116 U+5DF1 1513 2500 127 117 U+3082 1213 1523 72 119 U+3055 1219 3266 95 124 U+306E 259 2230 68 125 U+306E 1184 2423 169 125 U+4E16 849 2236 163 127 U+7D30 1144 1212 200 128 U+305D 316 3287 57 133 U+4EBA 217 2044 183 135 U+3051 277 2974 112 137 U+308C 201 3423 181 137 U+3060 243 2830 159 143 U+5F37 1479 2034 163 145 U+306E 1497 1567 123 152 U+305F 1164 952 145 153 U+3066 552 1199 97 155 U+4FF3 537 2095 176 155 U+6839 203 1439 184 156 U+304B 1188 2606 156 157 U+8AE7 549 2328 156 159 U+308C 1495 2784 168 159 U+5B50 891 1255 100 164 U+3092 584 2546 117 164 U+53CA 849 1588 151 164 U+8005 1192 2198 133 169 U+305A 889 1763 103 171 U+907F 513 945 181 171 U+6B63 539 1439 136 172 U+6587 192 2382 216 173 U+3075 1512 3371 147 176 U+6642 1465 1338 168 179 U+601D 1492 3175 159 180 U+306A 1191 2775 135 181 U+3081 593 3313 151 184 U+6D6E 868 1982 155 184 U+3092 873 2400 145 192 U+6C17 1504 1754 145 200 U+8077 208 1770 197 204 U+8001 1167 1687 152 208 U+6B66 1184 1942 171 208 U+697D 568 2762 133 209 U+3082 247 1159 116 212 U+76F2 253 2578 119 215 U+82E5 1465 951 172 216 U+81EA 1852 1736 104 219 U+3069 220 928 139 229 U+98A8 541 1619 147 236 U+306B 1521 2239 83 237 U+88CF 851 2608 169 237 U+7573 905 3189 103 244 U+606F 876 937 123 244 U+5E8F 1816 2096 152 296 U+3057 629 2985 27 300 U+3057 1243 2942 39 313\"\n",
    "    results = data.split(\" \")\n",
    "    results = list(chunks(results, 5))\n",
    "\n",
    "    for result in results:\n",
    "        char, x, y, w, h = result\n",
    "        x, y, w, h = int(x), int(y), int(w), int(h)\n",
    "        img = cv2.rectangle(img, (x, y), (x+w, y+h), (255,255,255), 3)\n",
    "\n",
    "    return img\n",
    "\n",
    "img1 = VisualizeTraining(DIR + \"train_images/100241706_00004_2.jpg\")\n",
    "# img1 = VisualizeKuzushiji(DIR + \"train/100241706_00004_2.jpg\")\n",
    "\n",
    "\n",
    "# plt.subplot(1,4,2),\n",
    "# plt.title(\\\"Recognition of Kuzushiji\\\",fontsize=20),\n",
    "# plt.imshow(imsource1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_id    0\n",
      "labels      0\n",
      "dtype: int64\n",
      "0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100241706_00004_2</td>\n",
       "      <td>U+306F 1231 3465 133 53 U+304C 275 1652 84 69 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100241706_00005_1</td>\n",
       "      <td>U+306F 1087 2018 103 65 U+304B 1456 1832 40 73...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100241706_00005_2</td>\n",
       "      <td>U+306F 572 1376 125 57 U+306E 1551 2080 69 68 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100241706_00006_1</td>\n",
       "      <td>U+3082 1455 3009 65 44 U+516B 1654 1528 141 75...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100241706_00007_2</td>\n",
       "      <td>U+309D 1201 2949 27 33 U+309D 1196 1539 27 36 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            image_id                                             labels\n",
       "0  100241706_00004_2  U+306F 1231 3465 133 53 U+304C 275 1652 84 69 ...\n",
       "1  100241706_00005_1  U+306F 1087 2018 103 65 U+304B 1456 1832 40 73...\n",
       "2  100241706_00005_2  U+306F 572 1376 125 57 U+306E 1551 2080 69 68 ...\n",
       "3  100241706_00006_1  U+3082 1455 3009 65 44 U+516B 1654 1528 141 75...\n",
       "4  100241706_00007_2  U+309D 1201 2949 27 33 U+309D 1196 1539 27 36 ..."
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read in the unicode translation\n",
    "unicode_map = {codepoint: char for codepoint, char in pd.read_csv(DIR + '/unicode_translation.csv').values}\n",
    "integer_map = {codepoint: i for i, codepoint in enumerate(unicode_map.keys())}\n",
    "\n",
    "# Read in the training data\n",
    "data = pd.read_csv(DIR + '/train.csv')\n",
    "\n",
    "# Check for null values\n",
    "print(data.isnull().sum())\n",
    "\n",
    "# Check for duplicates\n",
    "print(data.duplicated().sum())\n",
    "\n",
    "# Remove characters that don't have a unicode translation\n",
    "def filter_labels(row, valid_keys):\n",
    "    elements = row['labels'].split(' ')\n",
    "    split_elements = [elements[i:i+5] for i in range(0, len(elements), 5)]\n",
    "    filtered_elements = []\n",
    "\n",
    "    for element in split_elements:\n",
    "        if element[0] in valid_keys:\n",
    "            filtered_elements.append(element)\n",
    "\n",
    "    return \" \".join(np.asarray(filtered_elements).flatten())\n",
    "    \n",
    "data['labels'] = data.apply(lambda row: filter_labels(row, unicode_map.keys()), axis=1)\n",
    "\n",
    "# Display sample data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import random as rand\n",
    "import matplotlib as mpl\n",
    "\n",
    "# def extract_data():\n",
    "#    '''\n",
    "#    Takes in training data, segments the characters, and returns a list of character bounding boxes with encoding \n",
    "#    '''\n",
    "\n",
    "#    X = []\n",
    "#    Y = []   \n",
    "\n",
    "#    for image_encoding in tqdm(data[0:10].values):   \n",
    "#       try: \n",
    "#          # Clean data to be individal images \n",
    "#          seg_encoding = image_encoding[1].split(\" \")\n",
    "#          seg_encoding = [seg_encoding[i:i+5] for i in range(0, len(seg_encoding), 5)]\n",
    "\n",
    "#          # print(seg_encoding)\n",
    "\n",
    "#          # Read in the image and convert to threshold\n",
    "#          img = Image.open(DIR + \"/train_images/\" + image_encoding[0] + \".jpg\").convert(\"RGBA\")\n",
    "   \n",
    "#          for char, x, y, w, h in seg_encoding:\n",
    "            \n",
    "#             # Threshold each character and get encoding\n",
    "#             x,y,w,h = int(x), int(y), int(w), int(h)\n",
    "#             char = unicode_map[char]\n",
    "#             cropped_img = img.crop((x-10, y-10, x+w+10, y+h+10))\n",
    "#             resized_img = cropped_img.resize((256, 256))\n",
    "#             resized_img = np.array(resized_img)\n",
    "#             ret, im_th = cv2.threshold(cv2.cvtColor(resized_img, cv2.COLOR_BGR2GRAY), 130, 255, cv2.THRESH_BINARY_INV)\n",
    "            \n",
    "#             # Append to X and Y\n",
    "#             try:\n",
    "#                X.append(im_th)\n",
    "#             except Exception as E:\n",
    "#                print(E)\n",
    "#                print(f\"{[char, x, y, w, h]} with image encoding {image_encoding[0]} failed to append to X\")\n",
    "#                pass\n",
    "\n",
    "#             try:    \n",
    "#                Y.append(char)\n",
    "#             except Exception as E:\n",
    "#                print(E)\n",
    "#                X.pop()\n",
    "#                print(f\"{[char, x, y, w, h]} with image encoding {image_encoding[0]} failed to append to Y\")\n",
    "#                pass\n",
    "\n",
    "#       except Exception as E:\n",
    "#          print(E)\n",
    "\n",
    "#    return X, Y\n",
    "\n",
    "\n",
    "def display_sample(X, Y,  num_samples=20):\n",
    "   '''\n",
    "   Takes a random subsample of characters and displays them\n",
    "   '''\n",
    "\n",
    "   if num_samples > 200:\n",
    "      print(\"Please enter a number less than 200\")\n",
    "      return\n",
    "   \n",
    "   indicies = rand.sample(range(0, len(X)), num_samples)\n",
    "   X_samples = [X[i] for i in indicies]   \n",
    "   Y_samples = [Y[i] for i in indicies]\n",
    "\n",
    "   # Change font to allow Japaense characters\n",
    "   plt.figure(figsize=(20,20))\n",
    "   mpl.rcParams['font.family'] = 'Noto Sans CJK JP'\n",
    "\n",
    "   for i, sample in enumerate(range(num_samples)):\n",
    "      plt.subplot(4, 5, i+1)\n",
    "      plt.title(Y_samples[i])\n",
    "      plt.imshow(X_samples[i])\n",
    "\n",
    "\n",
    "# X_train, Y_train = extract_data()\n",
    "# display_sample(X_train, Y_train)\n",
    "# print(len(X_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def extract_data(img_encoding, img_size):\n",
    "   '''\n",
    "   Takes in an image_file & its char encoding, returning all the characters & labels for that image \n",
    "   '''\n",
    "\n",
    "   X = []\n",
    "   Y = []   \n",
    "\n",
    "   try:\n",
    "      # Clean data to be individal images\n",
    "      seg_encoding = img_encoding[1].split(\" \")\n",
    "      seg_encoding = [seg_encoding[i:i+5] for i in range(0, len(seg_encoding), 5)]\n",
    "      \n",
    "      # Shuffle encoding to ignore ordering\n",
    "      random.shuffle(seg_encoding)\n",
    "\n",
    "      # Read in the image and convert to threshold\n",
    "      img = Image.open(DIR + \"/train_images/\" + img_encoding[0] + \".jpg\").convert(\"RGBA\")\n",
    "\n",
    "      # Loop through each character and get encoding\n",
    "      for char, x, y, w, h in seg_encoding:         \n",
    "         x,y,w,h = int(x), int(y), int(w), int(h)\n",
    "         # use integer_map instead of unique_code map for SCCE\n",
    "         char = integer_map[char]\n",
    "         cropped_img = img.crop((x-10, y-10, x+w+10, y+h+10))\n",
    "         resized_img = cropped_img.resize((img_size, img_size))\n",
    "         resized_img = np.array(resized_img)\n",
    "         ret, im_th = cv2.threshold(cv2.cvtColor(resized_img, cv2.COLOR_BGR2GRAY), 130, 255, cv2.THRESH_BINARY_INV)\n",
    "\n",
    "         # Append images and labels to X and Y\n",
    "         try:\n",
    "            X.append(im_th)\n",
    "         except Exception as E:\n",
    "            print(E)\n",
    "            print(f\"{[char, x, y, w, h]} with image encoding {img_encoding[0]} failed to append to X\")\n",
    "            pass\n",
    "\n",
    "         try:    \n",
    "            Y.append(char)\n",
    "         except Exception as E:\n",
    "            print(E)\n",
    "            X.pop()\n",
    "            print(f\"{[char, x, y, w, h]} with image encoding {img_encoding[0]} failed to append to Y\")\n",
    "            pass\n",
    "         \n",
    "   except Exception as E:\n",
    "      print(f\"There was an exception with: {E}\")\n",
    "   \n",
    "   X = np.asarray(X, dtype='float32')\n",
    "   Y = np.asarray(Y, dtype='int32')\n",
    "\n",
    "   return X, Y\n",
    "\n",
    "\n",
    "def data_generator(data, batch_size, total_batches, img_size):\n",
    "   '''Creates a generator that returns a batch of data'''\n",
    "   \n",
    "   # Create random indicies for image_data\n",
    "   indicies = rand.sample(range(0, len(data)), len(data))\n",
    "   \n",
    "   X_batch, Y_batch = [], []\n",
    "   \n",
    "   # Assert that the number of batches is correct\n",
    "   # assert sum([len(data.iloc[i]['labels']) // 5 for i in data]) // batch_size + 1 == total_batches\n",
    "\n",
    "   while True:\n",
    "      mnspt_idx = 0\n",
    "\n",
    "      # Create batch\n",
    "      for batch_num in range(total_batches):\n",
    "         \n",
    "         while(mnspt_idx < len(data) and len(X_batch) < batch_size):   \n",
    "            # Extract next manuscript\n",
    "            X_temp, Y_temp = extract_data(data.values[indicies[mnspt_idx]], img_size)\n",
    "            \n",
    "            # print(f'X_temp type: {type(X_temp)} at {data.values[indicies[mnspt_idx]]}')\n",
    "\n",
    "            if(len(X_batch) == 0):\n",
    "               X_batch = X_temp\n",
    "               Y_batch = Y_temp\n",
    "            \n",
    "            else:\n",
    "               X_batch = np.concatenate((X_batch, X_temp), axis=0)\n",
    "               Y_batch = np.concatenate((Y_batch, Y_temp), axis=0)\n",
    "               \n",
    "            # Increment manuscript\n",
    "            mnspt_idx = mnspt_idx + 1\n",
    "            # print(f'MNSPT_IDX: {mnspt_idx}')\n",
    "                  \n",
    "         # Seperate batch & remaining characters \n",
    "         X_temp = X_temp[batch_size:]\n",
    "         Y_temp = Y_temp[batch_size:]\n",
    "         \n",
    "         X_result = X_batch[:batch_size]\n",
    "         Y_result = Y_batch[:batch_size]\n",
    "      \n",
    "         X_batch = X_temp\n",
    "         Y_batch = Y_temp\n",
    "\n",
    "         # print(\"X_batch dtype: \", X_batch.dtype, \" Y_batch dtype: \", Y_batch.dtype)\n",
    "\n",
    "         yield X_result, Y_result\n",
    "\n",
    "         if mnspt_idx >= len(data):\n",
    "            mnspt_idx = 0\n",
    "            print(\"Resetting mnspt_idx\")\n",
    "            rand.shuffle(indicies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "   # def collect_training(dataPath, trainImagePath):\n",
    "   #    imageNames = os.listdir(os.path.join(DIR + trainImagePath))\n",
    "   #    df = pd.read_csv(os.path.join(DIR + dataPath))\n",
    "      \n",
    "   #    # Check if there is a directory called \\\"training_images\\\" if there is, empty it otherwise make one,\n",
    "   #    training_dir = os.path.join(DIR + \"/segmented_training_images\")\n",
    "   #    if os.path.exists(training_dir):\n",
    "   #       for filename in os.listdir(training_dir):\n",
    "   #          file_path = os.path.join(training_dir, filename)\n",
    "   #          try:\n",
    "   #             os.remove(file_path)\n",
    "   #          except Exception as e:\n",
    "   #             print('Failed to delete %s. Reason: %s' % (file_path, e))\n",
    "   #          else:\n",
    "   #             os.makedirs(training_dir)\n",
    "\n",
    "   #          print(imageNames)      print(E)\n",
    "\n",
    "   #             rects = [chars[i:i+5] for i in range(0, len(chars), 5)]\n",
    "\n",
    "   #             for i, rect in enumerate(rects):\n",
    "   #                # Save a file for each character\n",
    "   #                char, x, y, w, h = rect\n",
    "   #                char_img = img[int(y):int(y)+int(h), int(x):int(x)+int(w)]\n",
    "   #                cv2.imwrite(os.path.join(training_dir + \" \", f\"{ID}_{i}_{char}.jpg\"), char_img)\n",
    "                              \n",
    "   #    # FILEPATH: /home/harris/Projects/AI-Stuff/AI-foundations/Competitions/Kuzushiji-Recognition-Comp/clean-data.ipynb\n",
    "   #    collect_training(\"train.csv\", \"train_images\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1721 424\n",
      "2307 577\n",
      "3605\n",
      "440532\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Segmenting data to lower training cost\n",
    "SEGMENT_DATA = len(data)\n",
    "BATCH_SIZE = 256\n",
    "IMAGE_SIZE = 128\n",
    "\n",
    "indicies = rand.sample(range(0, len(data)), SEGMENT_DATA)\n",
    "\n",
    "# Splitting training, validation and test data\n",
    "train_indices, test_indices = train_test_split(indicies, test_size=0.2, random_state=42)\n",
    "train_indices, val_indices = train_test_split(train_indices, test_size=0.2, random_state=42)\n",
    "\n",
    "# Calculate the number of steps per epoch\n",
    "train_steps = sum([len(data.iloc[i]['labels'].split(\" \")) // 5 for i in train_indices]) // BATCH_SIZE + 1\n",
    "validation_steps = sum([len(data.iloc[i]['labels'].split(\" \")) // 5 for i in val_indices]) // BATCH_SIZE + 1\n",
    "\n",
    "# Creating generators for training and validation data\n",
    "training_gen = data_generator(data.iloc[train_indices], BATCH_SIZE, train_steps, IMAGE_SIZE)\n",
    "validation_gen = data_generator(data.iloc[val_indices], BATCH_SIZE, validation_steps, IMAGE_SIZE)\n",
    "\n",
    "print(train_steps, validation_steps)\n",
    "print(len(train_indices), len(val_indices))\n",
    "print(SEGMENT_DATA)\n",
    "print(sum([len(data.iloc[i]['labels'].split(\" \")) // 5 for i in train_indices]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_10 (InputLayer)       [(None, 128, 128, 1)]     0         \n",
      "                                                                 \n",
      " conv2d_14 (Conv2D)          (None, 128, 128, 32)      320       \n",
      "                                                                 \n",
      " max_pooling2d_14 (MaxPooli  (None, 64, 64, 32)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_15 (Conv2D)          (None, 64, 64, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_15 (MaxPooli  (None, 32, 32, 64)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " global_average_pooling2d_9  (None, 64)                0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 256)               16640     \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 4781)              1228717   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1264173 (4.82 MB)\n",
      "Trainable params: 1264173 (4.82 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Creating model\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "NUM_CLASSES = len(unicode_map.keys())\n",
    "\n",
    "inputs = keras.Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 1))\n",
    "x = layers.Conv2D(32, (3,3), padding='same', activation='relu')(inputs)\n",
    "x = layers.MaxPooling2D((2,2))(x)\n",
    "x = layers.Conv2D(64, (3,3), padding='same', activation='relu')(x)\n",
    "x = layers.MaxPooling2D((2,2))(x)\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "x = layers.Dense(256, activation='relu')(x)# Calculate the number of steps per epoch\n",
    "\n",
    "outputs = layers.Dense(NUM_CLASSES, activation='softmax')(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1024, 128, 128)\n",
      "(1024, 128, 128)\n"
     ]
    }
   ],
   "source": [
    "x_batch, y_batch = next(training_gen)\n",
    "print(x_batch.shape)  # This should show something like (batch_size, height, width, channels)\n",
    "\n",
    "x_batch, y_batch = next(validation_gen)\n",
    "print(x_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   # Replace with your generator and its parameters\n",
    "# total_batches = train_steps # Total number of batches\n",
    "# batch_to_start_inspection = total_batches - 5  # Start inspection 5 batches before the end\n",
    "\n",
    "# for i in range(total_batches):\n",
    "#     try:\n",
    "#         print(f\"Batch {i} loaded successfully\")\n",
    "#         X_batch, Y_batch = next(training_gen)\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error processing batch {i}: {e}\")\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1262/1721 [====================>.........] - ETA: 1:29 - loss: 4.8885 - accuracy: 0.1370Resetting mnspt_idx\n",
      "1721/1721 [==============================] - ETA: 0s - loss: 4.6682 - accuracy: 0.1589Resetting mnspt_idx\n",
      "1721/1721 [==============================] - 417s 242ms/step - loss: 4.6682 - accuracy: 0.1589 - val_loss: 4.0781 - val_accuracy: 0.2377\n",
      "Epoch 2/10\n",
      "1258/1721 [====================>.........] - ETA: 1:30 - loss: 3.7909 - accuracy: 0.2646Resetting mnspt_idx\n",
      "1721/1721 [==============================] - ETA: 0s - loss: 3.7445 - accuracy: 0.2727Resetting mnspt_idx\n",
      "1721/1721 [==============================] - 418s 243ms/step - loss: 3.7445 - accuracy: 0.2727 - val_loss: 3.6024 - val_accuracy: 0.3045\n",
      "Epoch 3/10\n",
      "1253/1721 [====================>.........] - ETA: 1:32 - loss: 3.5158 - accuracy: 0.3157Resetting mnspt_idx\n",
      "1721/1721 [==============================] - ETA: 0s - loss: 3.4937 - accuracy: 0.3206Resetting mnspt_idx\n",
      "1721/1721 [==============================] - 418s 243ms/step - loss: 3.4937 - accuracy: 0.3206 - val_loss: 3.4742 - val_accuracy: 0.3408\n",
      "Epoch 4/10\n",
      "1270/1721 [=====================>........] - ETA: 1:23 - loss: 3.3467 - accuracy: 0.3487Resetting mnspt_idx\n",
      "1721/1721 [==============================] - ETA: 0s - loss: 3.3378 - accuracy: 0.3508Resetting mnspt_idx\n",
      "1721/1721 [==============================] - 398s 232ms/step - loss: 3.3378 - accuracy: 0.3508 - val_loss: 3.2998 - val_accuracy: 0.3589\n",
      "Epoch 5/10\n",
      "1270/1721 [=====================>........] - ETA: 1:22 - loss: 3.2452 - accuracy: 0.3715Resetting mnspt_idx\n",
      "1721/1721 [==============================] - ETA: 0s - loss: 3.2358 - accuracy: 0.3735Resetting mnspt_idx\n",
      "1721/1721 [==============================] - 396s 230ms/step - loss: 3.2358 - accuracy: 0.3735 - val_loss: 3.1838 - val_accuracy: 0.3857\n",
      "Epoch 6/10\n",
      "1258/1721 [====================>.........] - ETA: 1:24 - loss: 3.1705 - accuracy: 0.3867Resetting mnspt_idx\n",
      "1721/1721 [==============================] - ETA: 0s - loss: 3.1589 - accuracy: 0.3884Resetting mnspt_idx\n",
      "1721/1721 [==============================] - 398s 232ms/step - loss: 3.1589 - accuracy: 0.3884 - val_loss: 3.2126 - val_accuracy: 0.3882\n",
      "Epoch 7/10\n",
      "1259/1721 [====================>.........] - ETA: 1:22 - loss: 3.1181 - accuracy: 0.3970Resetting mnspt_idx\n",
      "1721/1721 [==============================] - ETA: 0s - loss: 3.1191 - accuracy: 0.3983Resetting mnspt_idx\n",
      "1721/1721 [==============================] - 401s 233ms/step - loss: 3.1191 - accuracy: 0.3983 - val_loss: 3.0480 - val_accuracy: 0.4090\n",
      "Epoch 8/10\n",
      "1256/1721 [====================>.........] - ETA: 1:10 - loss: 3.0915 - accuracy: 0.4062Resetting mnspt_idx\n",
      "1721/1721 [==============================] - ETA: 0s - loss: 3.0815 - accuracy: 0.4081Resetting mnspt_idx\n",
      "1721/1721 [==============================] - 359s 208ms/step - loss: 3.0815 - accuracy: 0.4081 - val_loss: 3.0801 - val_accuracy: 0.4154\n",
      "Epoch 9/10\n",
      "1269/1721 [=====================>........] - ETA: 1:09 - loss: 3.0709 - accuracy: 0.4122Resetting mnspt_idx\n",
      "1721/1721 [==============================] - ETA: 0s - loss: 3.0546 - accuracy: 0.4151Resetting mnspt_idx\n",
      "1721/1721 [==============================] - 369s 214ms/step - loss: 3.0546 - accuracy: 0.4151 - val_loss: 3.0657 - val_accuracy: 0.4095\n",
      "Epoch 10/10\n",
      "1266/1721 [=====================>........] - ETA: 1:10 - loss: 3.0447 - accuracy: 0.4174Resetting mnspt_idx\n",
      "1721/1721 [==============================] - ETA: 0s - loss: 3.0467 - accuracy: 0.4185Resetting mnspt_idx\n",
      "1721/1721 [==============================] - 362s 210ms/step - loss: 3.0467 - accuracy: 0.4185 - val_loss: 2.9989 - val_accuracy: 0.4268\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fcb4334d2d0>"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit and compile model\n",
    "model.compile(\n",
    "    optimizer='rmsprop',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    training_gen,\n",
    "    steps_per_epoch=train_steps,\n",
    "    validation_data=validation_gen,\n",
    "    validation_steps=validation_steps,\n",
    "    epochs=10\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
